ğŸš€ Real-Time YOLOv11 Video Inference â€“ Production MLOps Demo
Overview

This project demonstrates a production-grade, real-time computer vision inference system built with YOLOv11, deployed as a scalable cloud service, and consumed directly from a web browser camera.

The system showcases end-to-end MLOps principles, including model serving, cloud deployment, latency benchmarking, frontend-backend separation, and real-time visualization.

ğŸ¯ Target audience:
Hiring managers, recruiters, and engineering teams evaluating real-world MLOps & inference skills.

ğŸ§  System Architecture
Browser (Webcam)
   â”‚
   â”‚  JPEG frames (HTTPS)
   â–¼
Frontend (Static Website)
   â”‚
   â”‚  POST /predict
   â–¼
Inference API (Google Cloud Run)
   â”‚
   â”‚  YOLOv11 (ONNX Runtime)
   â–¼
JSON detections â†’ Bounding boxes â†’ FPS overlay

Key Properties

Stateless inference service

Horizontally scalable

HTTPS end-to-end

Real-time visualization in browser

Cloud-native deployment

ğŸ—ï¸ Tech Stack
Model & Inference

YOLOv11 (Ultralytics)

ONNX export

ONNX Runtime (CPU)

Input size: 640Ã—640

Output format: (1, 4 + num_classes, 8400)

Backend

Python 3.10

FastAPI

Uvicorn

Docker

Google Cloud Run

Frontend

Vanilla HTML / JavaScript

WebRTC camera access

Canvas rendering

Client-side FPS estimation

Cloud

Google Cloud Run (serverless)

Google Cloud Storage (model artifacts)

Container Registry

HTTPS + IAM-based access

ğŸ” Features
âœ… Real-Time Video Inference

Captures webcam frames in browser

Sends frames to inference endpoint

Draws bounding boxes and class labels

âœ… End-to-End Latency Measurement

Client-side FPS estimation

Server-side inference timing

Full round-trip latency display

âœ… Production-Style API Design

Stateless /predict endpoint

JSON response with structured detections

Designed for batching & scaling

âœ… Cloud-Native Deployment

Dockerized inference service

Deployed on Cloud Run

HTTPS by default

Autoscaling enabled

ğŸ“Š Performance Benchmarks (CPU â€“ Cloud Run)
Metric	Value
Input resolution	640Ã—640
Model	YOLOv11 ONNX
Inference latency	~80â€“100 ms
End-to-end latency	~120â€“180 ms
Effective FPS	~5â€“7 Hz
Cold start	1â€“4 s (mitigated via warm instances)
ğŸ§ª Latency Instrumentation

Latency is measured at multiple levels:

Backend

Preprocessing time

Inference time

Total server processing time

Frontend

Request â†’ response round-trip

FPS estimation using rolling window

This allows accurate bottleneck analysis and optimization planning.

ğŸ” Security & Deployment Notes

HTTPS enforced (required for camera access)

CORS restricted to frontend domain

Cloud Run deployed with --allow-unauthenticated (demo scope)

Model artifacts stored in Cloud Storage

ğŸ§  MLOps Concepts Demonstrated

Model export & optimization (ONNX)

Containerized inference

Serverless deployment

Latency benchmarking

Client/server separation

Stateless inference design

Real-time constraints

Cloud scalability tradeoffs

ğŸš€ Possible Extensions

GPU-backed inference (GKE or Cloud Run GPU)

Micro-batching for throughput optimization

WebSocket or WebRTC streaming

Authentication & rate limiting

Model versioning & A/B testing

CI/CD pipeline for model rollout



ğŸ‘¤ Author

Ben Yosef Weiss
AI / ML / Computer Vision Engineer
ğŸ”— https://www.benyosefweiss.com